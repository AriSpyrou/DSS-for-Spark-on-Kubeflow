# // Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# // SPDX-License-Identifier: MIT-0
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: tpcds-data-generation
  namespace: spark-operator
spec:
  type: Scala
  mode: cluster
  image: ghcr.io/aws-samples/eks-spark-benchmark:3.1.2
  imagePullPolicy: IfNotPresent
  sparkVersion: 3.1.2                    
  mainClass: com.amazonaws.eks.tpcds.DataGeneration
  mainApplicationFile: local:///opt/spark/examples/jars/eks-spark-benchmark-assembly-1.0.jar
  hadoopConf:
    fs.default.name: "hdfs://aspyrou-1:9000"
  arguments:
  # TPC-DS data localtion
    - "hdfs:///tpcds-data"
  # Path to kit in the docker image
    - "/opt/tpcds-kit/tools"
  # Data Format
    - "parquet"
  # Scale factor (in GB)
    - "10"
  # Generate data num partitions
    - "5"
  # Create the partitioned fact tables
    - "true"
  # Shuffle to get partitions coalesced into single files.
    - "true"
  # Logging set to WARN
    - "true"
  sparkConf:
    "spark.network.timeout": "2000s"
    "spark.executor.heartbeatInterval": "300s"
    "spark.kubernetes.memoryOverheadFactor": "0.3"
    "spark.sql.files.maxRecordsPerFile": "30000000"
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
  driver:
    cores: 1
    coreLimit: "3000m"
    memory: "512m"
    serviceAccount: "spark-spark"
    volumeMounts:
      - name: "spark-local-dir-1"
        mountPath: "/tmp/spark-local-dir "          
  executor:
    cores: 1
    coreLimit: "3000m"
    memory: "3500m"     
    instances: 5
    volumeMounts:
      - name: "spark-local-dir-1"
        mountPath: "/tmp/spark-local-dir"
  volumes:    
    - name: "spark-local-dir-1"
      emptyDir: {}
  restartPolicy:  
    type: Never